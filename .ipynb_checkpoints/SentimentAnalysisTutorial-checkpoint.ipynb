{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords, state_union\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing words and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "\n",
    "Form of grouping stuff\n",
    "\n",
    "* Word Tokenizers - separates by word\n",
    "* Sentence Tokenizers - separates by sentence\n",
    "\n",
    "### Corpora\n",
    "\n",
    "body of text. eg medical journals, presidential speeches\n",
    "\n",
    "### Lexicon\n",
    "dictionary - words and their meanings\n",
    "investor speak vs english\n",
    "investor speak - slangs \"bull on the market\"\n",
    "english - bull: an animal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"Liverpool will annihilate Chelsea. Easily a 4-0 victory for Liverpool.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenization\n",
    "Split by space.\n",
    "\n",
    "#### Sentence Tokenization\n",
    "Could use split punctuation, but could trip you out. Regex will be a pain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Liverpool will annihilate Chelsea.', 'Easily a 4-0 victory for Liverpool.']\n",
      "['Liverpool', 'will', 'annihilate', 'Chelsea', '.', 'Easily', 'a', '4-0', 'victory', 'for', 'Liverpool', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Words\n",
    "\n",
    "English has a lot of filler words that appear very frequently like “and”, “the”, and “a”. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other word. Stop words are usually identified by just by checking a hardcoded list of known stop words. But there’s no standard list of stop words that is appropriate for all applications. The list of words to ignore can vary depending on your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"it's\", 'if', 'needn', \"shouldn't\", 'hadn', 'no', \"didn't\", 'on', 'how', 'too', 'has', 're', 'couldn', 'd', 'an', 'by', \"shan't\", \"should've\", \"you're\", 'we', 'her', 'as', 'to', 'll', 'these', 'while', 'is', \"haven't\", \"that'll\", \"aren't\", 'that', 'did', 'weren', 'between', 'can', \"hadn't\", 'had', 'itself', 'but', 'being', 'the', 'are', 'over', 'same', \"weren't\", 'who', 'yourself', 'both', 'below', 'then', 'ours', 'with', 'after', 'theirs', 'do', 'down', 'will', 'should', 'ma', 'its', 'until', 'any', \"mightn't\", 'doing', 'because', 'most', 'again', 'not', 'having', 'them', 'am', \"isn't\", 'shouldn', 'for', 'more', 'were', 'you', \"wouldn't\", 'won', 'a', 'been', 'so', 'into', 'isn', 'where', 'don', 'those', 'this', 'about', 'your', 'than', 'and', 'it', 'which', 'above', 'what', 'nor', \"hasn't\", 'all', 'few', 'me', 'further', 'such', 'o', 'up', 'our', 'haven', \"mustn't\", \"don't\", 'their', 'here', 'whom', 'he', 'she', \"you've\", 'him', 'of', 'against', 'there', \"couldn't\", 'why', 'yourselves', \"doesn't\", 'hers', 'under', 'own', 'each', \"wasn't\", 'i', 'out', 'before', 'aren', \"you'll\", 'or', 't', 'myself', 'themselves', 'be', 'some', 'doesn', 'wasn', 'just', 'they', 'have', 'wouldn', 've', 'in', 'once', 'other', 'ain', \"won't\", 'only', 'ourselves', 'during', 'does', 'off', 'm', 'hasn', 'didn', 'when', 'from', 'mustn', 'very', 'now', \"you'd\", 'was', 's', 'my', 'shan', 'yours', 'y', \"she's\", 'through', 'at', 'mightn', \"needn't\", 'himself', 'herself', 'his'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Liverpool',\n",
       " 'annihilate',\n",
       " 'Chelsea',\n",
       " '.',\n",
       " 'Easily',\n",
       " '4-0',\n",
       " 'victory',\n",
       " 'Liverpool',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in word_tokenize(example_text) if not w in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form of \"normalization\". Take words then take the stem of the word.\n",
    "for example - riding, ridden -- **root** is ride.\n",
    "\n",
    "We do this because we might have a variation of words but really the meaning of the sentence is really unchanged.\n",
    "\n",
    "I was taking a ride in the car\n",
    "\n",
    "I was riding in the car.\n",
    "\n",
    "two words having the same definition. Pointless, causes redundancy. \n",
    "\n",
    "PorterStemmer (circa 1979) used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destroy', 'destroy', 'destroy', 'destroy']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "example_words = [\"destroy\",\"destroyed\",\"destroying\",\"destroys\"]\n",
    "stemmed_words = [ps.stem(w) for w in example_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging\n",
    "labelling a part of speech to every word \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PunktSentenceTokenizer is an sentence boundary detection algorithm that must be trained to be used [1]. NLTK already includes a pre-trained version of the PunktSentenceTokenizer.\n",
    "\n",
    "So if you use initialize the tokenizer without any arguments, it will default to the pre-trained version\n",
    "\n",
    "You can also provide your own training data to train the tokenizer before using it. Punkt tokenizer uses an unsupervised algorithm, meaning you just train it with regular text.\n",
    "\n",
    "https://stackoverflow.com/questions/35275001/use-of-punktsentencetokenizer-in-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "## sentence tokenizer\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged = [nltk.pos_tag(nltk.word_tokenize(i)) for i in tokenized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking\n",
    "\n",
    "Who is the sentence talking about. Named enity (many nouns) in the account? Words that modify that noun. Descriptive bunch of words surrounding that noun.\n",
    "\n",
    "Chunking is a process of extracting phrases from unstructured text. Instead of just simple tokens which may not represent the actual meaning of the text, its advisable to use phrases such as “South Africa” as a single word instead of ‘South’ and ‘Africa’ separate words.\n",
    "\n",
    "Can chunk to find noun phrases. United States of America needs to be together, President Bush should be together. Chunks help it keep it together.\n",
    "\n",
    "\n",
    "\n",
    "https://rikenshah.github.io/articles/natural-language-\n",
    "\n",
    "https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"President Obama Barack White House barked at the cat\"\n",
    "grammar = ('''\n",
    "    NP: {<DT>?<JJ>*<NN>} # NP\n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "tree = chunkParser.parse(tagged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking\n",
    "A chink is what we wish to remove from the chunk. Can Use regular expressions to remove unwanted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name Entity Recoginition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(words)\n",
    "namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing\n",
    "\n",
    "Similar to stemming. Real word; a synonym."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
